{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "475e7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from river import stream,tree,metrics\n",
    "import utils\n",
    "from encoding import prefix_bin\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sliding_window\n",
    "import datetime, time\n",
    "import importlib\n",
    "importlib.reload(sliding_window)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "74727620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch cuda setting\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "06c1d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './data/loan_baseline.pnml_noise_0.15_iteration_1_seed_614_sample.csv'\n",
    "\n",
    "\n",
    "dataset = stream.iter_csv(\n",
    "            file_name\n",
    "#             './data/loan_baseline.pnml_noise_0.15_iteration_1_seed_614_simple.csv',\n",
    "            )\n",
    "\n",
    "totallength = len(list(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "1fa38122",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stream.iter_csv(\n",
    "            file_name,\n",
    "            drop=['noise', 'lifecycle:transition', 'Variant', 'Variant index'],\n",
    "            )\n",
    "enctype = 'Index-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "010f9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pair = {\n",
    "'Case ID':'caseid',\n",
    "'Activity':'activity',\n",
    "# 'Resource':'resource',\n",
    "'Complete Timestamp':'ts',\n",
    "}\n",
    "catatars= ['activity']#,'resource']\n",
    "\n",
    "case_dict ={}\n",
    "training_models ={}\n",
    "\n",
    "casecount = 0\n",
    "rowcounter = 0\n",
    "resultdict ={}\n",
    "acc_dict ={}\n",
    "prefix_wise_window = {}\n",
    "prediction_result = {}\n",
    "graceperiod_finish=0\n",
    "finishedcases = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "d2ea8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding window for training setting\n",
    "window_size = 50\n",
    "retraining_size = 10\n",
    "training_window = sliding_window.training_window(window_size,retraining_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "8076cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(row_counting, total_length, start_time, interval=500):\n",
    "    if rowcounter%interval == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running cases: %s'%(len(case_dict)),\n",
    "             'Elapse time: %s mins'%(round((time.time()-start_time)/60, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "391935b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customdataset():\n",
    "    def __init__(self, dataset):\n",
    "        '''\n",
    "        Convert dataset to tensor\n",
    "        \n",
    "        Params\n",
    "        dataset_type: Type of dataset, trainset, validset, and testset\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "\n",
    "\n",
    "    def preprocessing(self):\n",
    "        self.x_data=self.dataset[0]\n",
    "        self.y_data=self.dataset[1]\n",
    "\n",
    "        x = self.x_data.to_numpy()\n",
    "        x = np.reshape(x, (x.shape[0],1, x.shape[1]))\n",
    "        y_set = sorted(set(self.y_data))\n",
    "        train_y =[]\n",
    "        for y in self.y_data:\n",
    "            train_y.append(y_set.index(y))\n",
    "\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float)\n",
    "        y_tensor = torch.tensor(train_y, dtype=torch.long)\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "    \n",
    "    def test_preprocessing(self):\n",
    "        self.x_data=self.dataset\n",
    "\n",
    "        x = self.x_data.to_numpy()\n",
    "        x = np.reshape(x, (x.shape[0],1, x.shape[1]))\n",
    "\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        return x_tensor\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "59e92c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_dropout(nn.Module):\n",
    "    def __init__(self, input_x):\n",
    "        \n",
    "        super(AE_dropout, self).__init__()\n",
    "        self.shape = input_x.shape\n",
    "        \n",
    "        hidden_factor_size = 0.2\n",
    "        self.h_dim = int(self.shape[0]*self.shape[1] *hidden_factor_size)\n",
    "        self.z_dim = int(self.h_dim * hidden_factor_size)\n",
    "        \n",
    "        if self.h_dim ==0:\n",
    "            self.h_dim =1\n",
    "        if self.z_dim ==0:\n",
    "            self.z_dim =1\n",
    "        self.fc1 = nn.Linear(self.shape[0]*self.shape[1], self.h_dim) #encode\n",
    "        self.fc2 = nn.Linear(self.h_dim, self.z_dim) #encode\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.z_dim, self.h_dim) #decode\n",
    "        self.fc4 = nn.Linear(self.h_dim, self.shape[0]*self.shape[1]) #decode\n",
    "\n",
    "        self.dout = nn.Dropout(p=0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #initialize weights\n",
    "#         nn.init.xavier_normal(self.fc1.weight, gain=np.sqrt(2))\n",
    "#         nn.init.xavier_normal(self.fc2.weight, gain=np.sqrt(2))\n",
    "#         nn.init.xavier_normal(self.fc3.weight, gain=np.sqrt(2))\n",
    "#         nn.init.xavier_normal(self.fc4.weight, gain=np.sqrt(2))\n",
    "\n",
    "    def encode(self, input_x):\n",
    "        # x --> dropout --> fc1 --> tanh ---> dropout --> fc2 --> z\n",
    "        dx = self.dout(input_x)\n",
    "        h = self.relu(self.fc1(input_x))\n",
    "        h = self.dout(h)\n",
    "        z = self.fc2(h)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, input_x):\n",
    "        # z --> dropout --> fc3 --> tanh --> dropout --> fc4 --> sigmoid --> x'\n",
    "        dz = self.dout(z)\n",
    "        h = self.relu(self.fc3(z))\n",
    "        h = self.dout(h)\n",
    "        recon_x = self.sigmoid(self.fc4(h))\n",
    "        return recon_x.view(input_x.size())\n",
    "    \n",
    "    def forward(self, input_x):\n",
    "        #flatten input and pass to encode\n",
    "        z = self.encode(input_x.view(-1, self.shape[0]*self.shape[1]))\n",
    "        return self.decode(z, input_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "e36cce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(df):\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    for c in columns:\n",
    "        if len(set(list(df[c]))) != 1:\n",
    "            max_c = max(list(df[c]))\n",
    "            min_c = min(list(df[c]))\n",
    "            normalized_colum = []\n",
    "            for i in list(df[c]):\n",
    "                normalized_i = (i-min_c)/(max_c - min_c)\n",
    "                normalized_colum.append(normalized_i)\n",
    "                \n",
    "            df[c] = normalized_colum\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "184516db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_stage(window, training_models):\n",
    "    '''\n",
    "    Manage training stage of streaming anomaly detection\n",
    "    ----------\n",
    "    Parameters\n",
    "    window: class training_window\n",
    "        Sliding window with training data\n",
    "    training_models: dict\n",
    "        Trained detector by prefix stored in. Default is randomforest\n",
    "    ----------\n",
    "    Return\n",
    "    training_models\n",
    "    '''\n",
    "    pw_window = training_window.prefix_wise_window()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    for x in pw_window:\n",
    "        input_x = pw_window[x][0]\n",
    "        input_x = min_max_scaler(input_x)\n",
    "        dataset = pw_window[x]\n",
    "        train_x,_ = Customdataset(dataset).preprocessing()\n",
    "        loss_list =[]\n",
    "        x_tensor = train_x[0]\n",
    "        model = AE_dropout(x_tensor).cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        optimizer.zero_grad()\n",
    "        previous_model =0\n",
    "        for i in range(10):\n",
    "            running_loss =0\n",
    "            for pos, x2 in enumerate(train_x):\n",
    "                \n",
    "                x_tensor = x2.cuda()\n",
    "                y_tensor = x2.cuda()\n",
    "                output = model(x_tensor)\n",
    "                loss = criterion(output, y_tensor)\n",
    "                running_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "#             if len(loss_list) ==0:\n",
    "#                 pass\n",
    "\n",
    "#             else:\n",
    "#                 if running_loss > np.mean(loss_list):\n",
    "#                     break\n",
    "\n",
    "            loss_list.append(running_loss)\n",
    "            previous_model = model\n",
    "            \n",
    "        if 'detector_%s'%(x) not in training_models:\n",
    "            training_models['detector_%s'%(x)] =[0,0]\n",
    "        training_models['detector_%s'%(x)][0] += 1\n",
    "        training_models['detector_%s'%(x)][1] = previous_model\n",
    "        \n",
    "        del x_tensor\n",
    "        del y_tensor\n",
    "        del previous_model\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    return training_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "eac65791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_activity_proba(last_event):\n",
    "    '''\n",
    "    Predict next activity prediction \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    last_event: case_bin\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    modelid, prediction\n",
    "    \n",
    "    '''\n",
    "    feature_matrix = prefix_wise_window['window_%s'%(last_event.prefix_length)][0].columns.values\n",
    "    current_event = utils.readjustment_training(last_event.encoded, feature_matrix)\n",
    "    current_event = pd.Series(current_event).to_frame().T\n",
    "    current_event_columns = current_event.columns.values\n",
    "    c_event = Customdataset(current_event).test_preprocessing().cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model = training_models['detector_window_%s'%(last_event.prefix_length)][1]\n",
    "        test_output = model(c_event)\n",
    "        output_df = pd.DataFrame(columns = current_event_columns)\n",
    "        output_df.loc[0,:] = test_output.cpu().data\n",
    "        \n",
    "        actidxlist = []\n",
    "        actlist = []\n",
    "        for x in range(len(current_event_columns)):\n",
    "            if 'activity_%s'%(str(last_event.prefix_length)) in current_event_columns[x]:\n",
    "                actidxlist.append(x)\n",
    "                actlist.append(current_event_columns[x])\n",
    "        \n",
    "        prediction = [output_df.iloc[0, actidxlist].tolist(), actlist]\n",
    "        modelid = training_models['detector_window_%s'%(last_event.prefix_length)][0]\n",
    "\n",
    "    return modelid, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "58a7a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_event(case_bin):\n",
    "    '''\n",
    "    Generate start event before first event\n",
    "    '''\n",
    "    print(case_bin.event['ts'])\n",
    "    empty_data ={'activity':'Start signal', 'ts':datetime.datetime.strftime(case_bin.event['ts'], '%Y-%m-%d %H:%M:%S')}\n",
    "    start_event = prefix_bin(case_bin.caseid, empty_data)\n",
    "    start_event.set_prefix_length(0)\n",
    "    start_event.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "    start_event.update_truelabel(case_bin.event['activity'])\n",
    "    return start_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "a17b8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "8751e005",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % Case finished: 0 Running cases: 0 Elapse time: 0.0 mins\n",
      "5.8 % Case finished: 29 Running cases: 1 Elapse time: 0.143 mins\n",
      "11.61 % Case finished: 60 Running cases: 1 Elapse time: 0.996 mins\n",
      "17.41 % Case finished: 92 Running cases: 1 Elapse time: 1.678 mins\n",
      "23.22 % Case finished: 121 Running cases: 1 Elapse time: 2.4 mins\n",
      "29.02 % Case finished: 148 Running cases: 1 Elapse time: 2.977 mins\n",
      "34.83 % Case finished: 180 Running cases: 0 Elapse time: 4.004 mins\n",
      "40.63 % Case finished: 211 Running cases: 1 Elapse time: 4.628 mins\n",
      "46.44 % Case finished: 242 Running cases: 1 Elapse time: 5.198 mins\n",
      "52.24 % Case finished: 273 Running cases: 0 Elapse time: 5.813 mins\n",
      "58.05 % Case finished: 301 Running cases: 1 Elapse time: 6.608 mins\n",
      "63.85 % Case finished: 334 Running cases: 0 Elapse time: 7.231 mins\n",
      "69.65 % Case finished: 365 Running cases: 1 Elapse time: 7.814 mins\n",
      "75.46 % Case finished: 395 Running cases: 1 Elapse time: 8.47 mins\n",
      "81.26 % Case finished: 427 Running cases: 1 Elapse time: 9.152 mins\n",
      "87.07 % Case finished: 457 Running cases: 1 Elapse time: 9.752 mins\n",
      "92.87 % Case finished: 490 Running cases: 1 Elapse time: 10.584 mins\n",
      "98.68 % Case finished: 518 Running cases: 1 Elapse time: 10.972 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for x,y in dataset:\n",
    "    display_progress(rowcounter, totallength, start_time)\n",
    "    rowcounter +=1\n",
    "    \n",
    "    utils.dictkey_chg(x, key_pair)\n",
    "    # Event stream change dictionary keys\n",
    "    x['ts'] = x['ts'][:-4]\n",
    "    \n",
    "    # Check label possible\n",
    "    \n",
    "    # Initialize case by prefix length\n",
    "    caseid = x['caseid']\n",
    "    x.pop('caseid')\n",
    "    \n",
    "    case_bin = prefix_bin(caseid, x)\n",
    "    \n",
    "    if caseid not in list(case_dict.keys()):\n",
    "        case_dict[caseid] = []\n",
    "        case_bin.set_prefix_length(1)\n",
    "        \n",
    "    elif caseid in finishedcases:\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        case_bin.set_prefix_length(len(case_dict[caseid])+1)\n",
    "        case_bin.set_prev_enc(case_dict[caseid][-1])\n",
    "    \n",
    "    # Encode event and cases and add to DB\n",
    "    ts = case_bin.event['ts']\n",
    "    case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "    \n",
    "    # Set current activity as outcome of previous event\n",
    "    if case_bin.prefix_length != 1:\n",
    "        case_bin.prev_enc.update_truelabel(x['activity'])\n",
    "\n",
    "    # First prediction for current event\n",
    "    \n",
    "    last_event = case_bin\n",
    "    modelid = 'None'\n",
    "    prediction = 'Not Available'\n",
    "\n",
    "    if len(training_window.getAllitems()) !=0:\n",
    "        if 'window_%s'%(last_event.prefix_length) in list(prefix_wise_window.keys()) and 'detector_window_%s'%(last_event.prefix_length) in training_models.keys():\n",
    "            modelid, prediction = predict_activity_proba(last_event)\n",
    "\n",
    "    case_bin.update_prediction((modelid, (prediction,ts)))        \n",
    "            \n",
    "    # Update training window and finish the case\n",
    "    if x['activity'] == 'End':\n",
    "        training_window.update_window({caseid: case_dict[caseid]})        \n",
    "        if training_window.retraining == training_window.retraining_count:            \n",
    "            train_start = time.time()\n",
    "            training_models = training_stage(training_window, training_models)\n",
    "            train_end = time.time()\n",
    "            training_time.append(train_end-train_start)\n",
    "            prefix_wise_window = training_window.prefix_wise_window()\n",
    "            \n",
    "        resultdict[caseid] = case_dict[caseid]\n",
    "        case_dict.pop(caseid)\n",
    "\n",
    "        casecount +=1\n",
    "        for x in case_dict:\n",
    "            last_event = case_dict[x][-1]\n",
    "            modelid = 'None'\n",
    "            prediction = 'Not Available'\n",
    "\n",
    "            if len(training_window.getAllitems()) !=0:\n",
    "                prefix_wise_window = training_window.prefix_wise_window()\n",
    "                if 'window_%s'%(last_event.prefix_length) in list(prefix_wise_window.keys()) and 'detector_window_%s'%(last_event.prefix_length) in training_models.keys():\n",
    "                    modelid, prediction = predict_activity_proba(last_event)\n",
    "            case_dict[x][-1].update_prediction((modelid, (prediction,ts)))     \n",
    "            \n",
    "        training_window.reset_retraining_count()\n",
    "    else:\n",
    "        case_dict[caseid].append(case_bin)\n",
    "\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "3ae84c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "78737e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, [[0.9735644459724426, 2.40995586864301e-06, 0.0037783682346343994, 3.510758688207716e-05, 0.0009369993931613863, 0.004429376684129238], ['activity_2 start_event_Loan  application received', 'activity_2 Return application back to applicant', 'activity_2 Approve application', 'activity_2 end_event_Loan application rejected', 'activity_2 Check if home insurance quote is requested', 'activity_2 end_event_Loan  application approved']])\n",
      "start_event_Loan  application received [[0.5359064340591431, 0.2783779501914978, 0.48349928855895996, 0.3379077911376953, 0.3861122727394104, 0.4223542809486389], ['activity_2 start_event_Loan  application received', 'activity_2 Return application back to applicant', 'activity_2 Approve application', 'activity_2 end_event_Loan application rejected', 'activity_2 Check if home insurance quote is requested', 'activity_2 end_event_Loan  application approved']]\n",
      "Normal\n"
     ]
    }
   ],
   "source": [
    "n= 10\n",
    "p = list(resultdict['49'][5].predicted.values())[0][0]\n",
    "p = resultdict['49'][5].__dict__\n",
    "true_act = resultdict['49'][1].event['activity']\n",
    "print(predict_activity_proba(resultdict['49'][1]))\n",
    "\n",
    "predictions = predict_activity_proba(resultdict['49'][1])[1]\n",
    "pos = 1\n",
    "predictions_proba = predictions[0]\n",
    "predictions_value = predictions[1]\n",
    "m_predictions_value = []\n",
    "prediction_label = 'Normal'\n",
    "\n",
    "for k in range(len(predictions_value)):\n",
    "    m_predictions_value.append(predictions_value[k].split('activity_%s '%(pos+1))[1])\n",
    "\n",
    "if true_act in m_predictions_value:\n",
    "    labelidx = m_predictions_value.index(true_act)\n",
    "\n",
    "    if predictions_proba[labelidx] <threshold:\n",
    "        prediction_label = 'Anomalous'\n",
    "else:\n",
    "    prediction_label = 'Anomalous'\n",
    "    \n",
    "print(true_act, predictions)\n",
    "print(prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "507d274c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Anomalous       0.16      0.93      0.27      1065\n",
      "       Normal       0.96      0.16      0.27      6497\n",
      "Not Available       0.00      0.00      0.00         0\n",
      "\n",
      "     accuracy                           0.27      7562\n",
      "    macro avg       0.37      0.36      0.18      7562\n",
      " weighted avg       0.84      0.27      0.27      7562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Anomalous       0.16      0.93      0.27      1065\n",
      "       Normal       0.96      0.15      0.26      6497\n",
      "Not Available       0.00      0.00      0.00         0\n",
      "\n",
      "     accuracy                           0.26      7562\n",
      "    macro avg       0.37      0.36      0.18      7562\n",
      " weighted avg       0.85      0.26      0.26      7562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Anomalous       0.16      0.94      0.27      1065\n",
      "       Normal       0.96      0.15      0.26      6497\n",
      "Not Available       0.00      0.00      0.00         0\n",
      "\n",
      "     accuracy                           0.26      7562\n",
      "    macro avg       0.37      0.36      0.18      7562\n",
      " weighted avg       0.85      0.26      0.26      7562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Anomalous       0.16      0.94      0.27      1065\n",
      "       Normal       0.97      0.15      0.26      6497\n",
      "Not Available       0.00      0.00      0.00         0\n",
      "\n",
      "     accuracy                           0.26      7562\n",
      "    macro avg       0.37      0.36      0.18      7562\n",
      " weighted avg       0.85      0.26      0.26      7562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Anomalous       0.16      0.94      0.27      1065\n",
      "       Normal       0.97      0.15      0.26      6497\n",
      "Not Available       0.00      0.00      0.00         0\n",
      "\n",
      "     accuracy                           0.26      7562\n",
      "    macro avg       0.38      0.36      0.18      7562\n",
      " weighted avg       0.85      0.26      0.26      7562\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Anomalous       0.16      0.94      0.27      1065\n",
      "       Normal       0.97      0.15      0.26      6497\n",
      "Not Available       0.00      0.00      0.00         0\n",
      "\n",
      "     accuracy                           0.26      7562\n",
      "    macro avg       0.38      0.36      0.18      7562\n",
      " weighted avg       0.86      0.26      0.26      7562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "counting_normal = 0\n",
    "\n",
    "for threshold in [0.01,0.05,0.1,0.15,0.2,0.25]:\n",
    "    global_true =[]\n",
    "    global_pred = []\n",
    "\n",
    "    for caseid in list(resultdict.keys()):\n",
    "\n",
    "        prediction_list = []\n",
    "\n",
    "        df = original_df[original_df['Case ID'] == int(caseid)].reset_index(drop=True)\n",
    "        for pos, t in enumerate(resultdict['%s'%(caseid)]):\n",
    "            if pos ==0:\n",
    "                continue\n",
    "            \n",
    "            true_act = t.event['activity']\n",
    "            prediction_label = 'Normal'\n",
    "            predictions = list(t.predicted.values())[0][0]\n",
    "            if predictions  == 'Not Available':\n",
    "                prediction_label = 'Not Available'\n",
    "            else:\n",
    "                predictions_proba = predictions[0]\n",
    "                predictions_value = predictions[1]\n",
    "                m_predictions_value = []\n",
    "                \n",
    "                for k in range(len(predictions_value)):\n",
    "                    m_predictions_value.append(predictions_value[k].split('activity_%s '%(pos+1))[1])\n",
    "                \n",
    "                if true_act in m_predictions_value:\n",
    "                    labelidx = m_predictions_value.index(true_act)\n",
    "\n",
    "                    if predictions_proba[labelidx] <threshold:\n",
    "                        prediction_label = 'Anomalous'\n",
    "                else:\n",
    "                    prediction_label = 'Anomalous'\n",
    "\n",
    "            if true_act != 'End':\n",
    "                prediction_list.append(prediction_label)\n",
    "\n",
    "#             if prediction_label == 'Anomalous':\n",
    "#                 print(true_act, m_predictions_value)\n",
    "            \n",
    "        true_label_list = []\n",
    "        labellist = list(df['noise'])\n",
    "        actlist = list(df['Activity'])\n",
    "        \n",
    "        for pos, t in enumerate(labellist):\n",
    "            if t == 'Start' or t == 'End':\n",
    "                continue\n",
    "            elif t == 'true':\n",
    "                true_label = 'Anomalous'\n",
    "            else:\n",
    "                true_label = 'Normal'\n",
    "            true_label_list.append(true_label)\n",
    "\n",
    "        for pos, p in enumerate(prediction_list):\n",
    "            global_pred.append(p)\n",
    "            global_true.append(true_label_list[pos])\n",
    "\n",
    "\n",
    "    saving_data = {'y_true':global_true, 'y_pred':global_pred}\n",
    "    import pickle\n",
    "    saving_file_name = file_name.split('/')[-1][:-4]\n",
    "    print(classification_report(saving_data['y_true'], saving_data['y_pred']))\n",
    "    with open('./result/ae_thr%s_window%s_retraining_%s_%s.pkl'%(threshold, window_size, retraining_size, saving_file_name), 'wb') as fp:\n",
    "        pickle.dump(saving_data, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43689c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
