{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d934ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789f85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = './gan_data'\n",
    "os.makedirs(path2data, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab23521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST(path2data, train=True,\n",
    "                         transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],\n",
    "                                                                                                   [0.5])]), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca88f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x128718ba190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOEElEQVR4nO3dcYgd5bnH8d+jNgibgBvFZU2iJiF/WIqaGkNBIyma4lVIUpDaiBcloVuwwSqiV1IxolRCrr3S/GF1a0OSSzUUTWkogTYNRW9Fa9Yl1axeo4aEJG6y2iAaQeKap3+cSVl1z3t2Z+acObvP9wPLnjPPzpmHIb/MnHnPnNfcXQAmvzOqbgBAaxB2IAjCDgRB2IEgCDsQxFmt3JiZcekfaDJ3t9GWFwq7mV0v6ZeSzpT0tLuvG8M6RTYJICE1lG55x9nN7ExJ+yQtkXRY0m5JK9z9zcQ6TtiB5nH3ukf2Iu/ZF0p61933u/tJSVslLSvwegCaqEjYZ0g6NOL54WzZl5hZj5n1mVlfgW0BKKjpF+jcvVdSr8QFOqBKRY7sRyTNGvF8ZrYMQBsqEvbdkuaZ2WwzmyLph5K2l9MWgLLlPo1392EzWy3pT6oNvW1094HSOgNQqtxDb7k2xtAb0FTNGnoDMIEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EETuKZsxMUyZMiVZnzVrVlO3v2/fvrq1c889N7nu0qVLk/WBgfQM4d3d3XVr27dvT6770UcfJeudnZ3JejvOVlwo7GZ2QNInkr6QNOzuC8poCkD5yjiyf9fdPyzhdQA0Ee/ZgSCKht0l/dnMXjOzntH+wMx6zKzPzPoKbgtAAUVP46929yNmdr6knWb2/+7+4sg/cPdeSb2SZGZecHsAcip0ZHf3I9nvIUm/l7SwjKYAlC932M2sw8ymnX4s6XuS9pbVGIBymXu+M2szm6Pa0VyqvR14xt1/3mAdb8fxx2a76qqrkvVp06Yl6/fee2+yft9999WtzZ49O7nuM888k6xX6ZVXXknWn3jiiWR9y5YtdWtDQ0O515Wkzz//PFlfu3Ztst4s7i53HzVkud+zu/t+SZfl7gpASzH0BgRB2IEgCDsQBGEHgiDsQBC5h95ybWySDr2tXLkyWV+zZk2yfuGFF5bZzoQxPDycrL/00kvJeqPhs5Tdu3cn6/v370/WG90iW5XU0BtHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Igq+SLsELL7yQrN90003JejuPs2/YsCFZP3DgQLL+8MMP162dOHEiue6SJUuSdYwPR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9hK89957yfoHH3yQrC9atChZ37RpU7I+d+7cZL3Ia69fvz5Zb3RPeareaL+hXBzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIvje+DcycOTNZf//995P1+fPn1629/PLLyXV37tyZrN94443JOtpLoe+NN7ONZjZkZntHLJtuZjvN7J3sd2eZDQMo31hO4zdJuv4ry+6XtMvd50nalT0H0MYaht3dX5R0/CuLl0nanD3eLGl5uW0BKFvez8Z3uftg9viopK56f2hmPZJ6cm4HQEkK3wjj7m5mda/yuXuvpF6pdoGu6PYA5JN36O2YmXVLUvY7/3SaAFoib9i3S7ote3ybpD+U0w6AZml4Gm9mz0paLOk8Mzssaa2kdZJ+Z2arJB2U9INmNjnZHT58uND6hw4dyr1uo3H4s85K/xNpNMc62kfDsLv7ijqla0vuBUAT8XFZIAjCDgRB2IEgCDsQBGEHguCrpCe51JTJkvTggw8m652d6Rsa77nnnnH3hGpwZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnnwRS0yK/+uqryXWPHj2arF955ZXJ+uLFi5P1p556qm7tkksuSa576tSpZB3jw5EdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgyubgtmzZkqxfc801yfoFF1yQe9uffvppsn7ttekvMO7v78+97cmq0JTNACYHwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnF2JN1yyy3JeqNx9kcffTT3tufPn5+sd3R0JOuN7uWfjAqNs5vZRjMbMrO9I5Y9ZGZHzGxP9nNDmQ0DKN9YTuM3Sbp+lOWPu/vl2c+OctsCULaGYXf3FyUdb0EvAJqoyAW61Wb2enaaX3dCMDPrMbM+M+srsC0ABeUN+68kzZV0uaRBSb+o94fu3uvuC9x9Qc5tAShBrrC7+zF3/8LdT0n6taSF5bYFoGy5wm5m3SOefl/S3np/C6A9NBxnN7NnJS2WdJ6kY5LWZs8vl+SSDkj6sbsPNtwY4+yTzkUXXZSsP/3003VrixYtSq57xhnpY9EDDzyQrK9fvz5Zn4xS4+wNJ4lw9xWjLP5N4a4AtBQflwWCIOxAEIQdCIKwA0EQdiAIbnFFZT7++ONk/eyzz07WP/vss2T98ccfr1tbu3Ztct2Jiq+SBkDYgSgIOxAEYQeCIOxAEIQdCIKwA0E0vOsNsd16663J+tatW5P1O+64o26t0Th6I88991yy/sgjjxR6/cmGIzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4+yS3dOnSZP3tt99O1u+8885kfePGjePuaaxOnjyZrPf39yfrw8PDZbYz4XFkB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEg+N74CeCyyy5L1nfs2FG3NjAwkFx38eLFeVoqxZNPPpmsd3R0JOurVq0qs51JodD3xpvZLDP7q5m9aWYDZvbTbPl0M9tpZu9kvzvLbhxAecZyGj8s6R53/6ak70j6iZl9U9L9kna5+zxJu7LnANpUw7C7+6C792ePP5H0lqQZkpZJ2pz92WZJy5vUI4ASjOuz8WZ2saT5kv4uqcvdB7PSUUldddbpkdRToEcAJRjz1XgzmyrpeUl3ufuXZuTz2lW+Ua/0uXuvuy9w9wWFOgVQyJjCbmbfUC3ov3X3bdniY2bWndW7JQ01p0UAZWg49Ga1sbLNko67+10jlv+3pH+6+zozu1/SdHe/r8FrhRx6u/TSS5P1m2++OVmfN29esr58+fLxtlSaDRs2JOvnn39+3drKlSuT63KL6vilht7G8p79Kkn/KekNM9uTLVsjaZ2k35nZKkkHJf2ghF4BNEnDsLv73yTVOxxfW247AJqFj8sCQRB2IAjCDgRB2IEgCDsQBLe4jtHcuXPr1q644orkurfffnuyft111+VpqRSPPfZYsj5jxoxk/e67707Wjx8/Pu6ekF+hW1wBTA6EHQiCsANBEHYgCMIOBEHYgSAIOxBEmCmbG009PDSU/u6N1atX160tXLgwV09lSY1lz5kzJ7nu1KlTk/VG+wUTB0d2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQgizP3s55xzTrLezPHkbdu2Jes9PenZsU6ePJmsd3WNOvOWJOngwYPJdTG5cD87AMIOREHYgSAIOxAEYQeCIOxAEIQdCGIs87PPkrRFUpckl9Tr7r80s4ck/UjSB9mfrnH3HQ1ea8J+bzwwEaTG2ccS9m5J3e7eb2bTJL0mablq87GfcPf0LANffi3CDjRRKuxjmZ99UNJg9vgTM3tLUnqaEABtZ1zv2c3sYknzJf09W7TazF43s41m1llnnR4z6zOzvmKtAihizJ+NN7Opkl6Q9HN332ZmXZI+VO19/COqneqvbPAanMYDTVToPbskmdk3JP1R0p/c/X9GqV8s6Y/u/q0Gr0PYgSYqdCOM1dL5G0lvjQx6duHutO9L2lu0UQDNM5ar8VdL+j9Jb0g6lS1eI2mFpMtVO40/IOnH2cW81GtxZAeaqPBpfFkIO9Bc3M8OgLADURB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxBEwy+cLNmH7j5yDuHzVPtqq3bUrr21a18SveVVZm8X1Su09H72r23crM/dF1TWQEK79taufUn0llereuM0HgiCsANBVB323oq3n9KuvbVrXxK95dWS3ip9zw6gdao+sgNoEcIOBFFJ2M3sejN728zeNbP7q+ihHjM7YGZvmNmequeny+bQGzKzvSOWTTeznWb2TvZ71Dn2KurtITM7ku27PWZ2Q0W9zTKzv5rZm2Y2YGY/zZZXuu8SfbVkv7X8PbuZnSlpn6Qlkg5L2i1phbu/2dJG6jCzA5IWuHvlH8Aws2sknZC05fTUWma2XtJxd1+X/UfZ6e7/1Sa9PaRxTuPdpN7qTTN+uyrcd2VOf55HFUf2hZLedff97n5S0lZJyyroo+25+4uSjn9l8TJJm7PHm1X7x9JydXprC+4+6O792eNPJJ2eZrzSfZfoqyWqCPsMSYdGPD+s9prv3SX92cxeM7OeqpsZRdeIabaOSuqqsplRNJzGu5W+Ms142+y7PNOfF8UFuq+72t2/Lek/JP0kO11tS157D9ZOY6e/kjRXtTkAByX9ospmsmnGn5d0l7t/PLJW5b4bpa+W7Lcqwn5E0qwRz2dmy9qCux/Jfg9J+r1qbzvaybHTM+hmv4cq7uff3P2Yu3/h7qck/VoV7rtsmvHnJf3W3bdliyvfd6P11ar9VkXYd0uaZ2azzWyKpB9K2l5BH19jZh3ZhROZWYek76n9pqLeLum27PFtkv5QYS9f0i7TeNebZlwV77vKpz/PZn1s6Y+kG1S7Iv+epJ9V0UOdvuZI+kf2M1B1b5KeVe207nPVrm2sknSupF2S3pH0F0nT26i3/1Vtau/XVQtWd0W9Xa3aKfrrkvZkPzdUve8SfbVkv/FxWSAILtABQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBD/AjKrxoHLfai/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = train_ds[0]\n",
    "plt.imshow(to_pil_image(img), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9fb792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "for x,y in train_dl:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db034c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.nz = params['nz']\n",
    "        self.img_size = params['img_size']\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *self._fc_layer(self.nz, 128, normalize=False),\n",
    "            *self._fc_layer(128,256),\n",
    "            *self._fc_layer(256,512),\n",
    "            *self._fc_layer(512,1024),\n",
    "            nn.Linear(1024, int(np.prod(self.img_size))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *self.img_size)\n",
    "        return img\n",
    "    \n",
    "    def _fc_layer(self, in_channels, out_channels, normalize=True):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_channels, out_channels))\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm1d(out_channels, 0.8))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return layers\n",
    "    \n",
    "params = {'nz': 100,\n",
    "         'img_size':(1,28,28)}\n",
    "x = torch.randn(16,100).to(device)\n",
    "model_gen = Generator(params).to(device)\n",
    "output = model_gen(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33aac93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.img_size = params['img_size']\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(self.img_size)), 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512,256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "x = torch.randn(16,1,28,28).to(device)\n",
    "model_dis = Discriminator(params).to(device)\n",
    "output = model_dis(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b6d6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    \n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0,0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)\n",
    "\n",
    "model_gen.apply(initialize_weights);\n",
    "model_dis.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7a7764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "lr = 2e-4\n",
    "beta1=0.5\n",
    "\n",
    "opt_dis = optim.Adam(model_dis.parameters(), lr=lr, betas = (beta1,0.999))\n",
    "opt_gen = optim.Adam(model_gen.parameters(), lr=lr, betas = (beta1,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11885f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label =1.\n",
    "fake_label =0.\n",
    "nz = params['nz']\n",
    "num_epochs = 100\n",
    "\n",
    "loss_history= {'gen':[],\n",
    "               'dis':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "810a1f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, G_Loss: 1.320275, D_Loss: 0.300906, time: 0.82 min\n",
      "Epoch: 1, G_Loss: 0.200445, D_Loss: 0.890927, time: 1.65 min\n",
      "Epoch: 1, G_Loss: 1.467200, D_Loss: 0.300173, time: 2.51 min\n",
      "Epoch: 2, G_Loss: 2.579363, D_Loss: 0.317045, time: 3.39 min\n",
      "Epoch: 2, G_Loss: 2.876921, D_Loss: 0.338064, time: 4.24 min\n",
      "Epoch: 3, G_Loss: 2.834472, D_Loss: 0.311743, time: 5.03 min\n",
      "Epoch: 3, G_Loss: 1.647935, D_Loss: 0.268050, time: 5.71 min\n",
      "Epoch: 4, G_Loss: 3.002031, D_Loss: 0.251153, time: 6.41 min\n",
      "Epoch: 4, G_Loss: 2.552371, D_Loss: 0.144602, time: 7.11 min\n",
      "Epoch: 5, G_Loss: 4.715493, D_Loss: 0.687775, time: 7.82 min\n",
      "Epoch: 5, G_Loss: 2.199317, D_Loss: 0.181209, time: 8.51 min\n",
      "Epoch: 6, G_Loss: 1.018975, D_Loss: 0.265983, time: 9.20 min\n",
      "Epoch: 6, G_Loss: 1.910239, D_Loss: 0.168536, time: 9.90 min\n",
      "Epoch: 7, G_Loss: 6.719062, D_Loss: 0.827666, time: 10.59 min\n",
      "Epoch: 7, G_Loss: 3.122081, D_Loss: 0.304061, time: 11.29 min\n",
      "Epoch: 8, G_Loss: 2.468286, D_Loss: 0.132911, time: 11.98 min\n",
      "Epoch: 9, G_Loss: 3.231904, D_Loss: 0.224038, time: 12.68 min\n",
      "Epoch: 9, G_Loss: 1.128583, D_Loss: 0.262357, time: 13.36 min\n",
      "Epoch: 10, G_Loss: 2.969755, D_Loss: 0.109639, time: 14.04 min\n",
      "Epoch: 10, G_Loss: 2.607861, D_Loss: 0.104309, time: 14.73 min\n",
      "Epoch: 11, G_Loss: 3.916423, D_Loss: 0.162034, time: 15.41 min\n",
      "Epoch: 11, G_Loss: 4.720711, D_Loss: 0.216635, time: 16.10 min\n",
      "Epoch: 12, G_Loss: 2.604356, D_Loss: 0.258086, time: 16.79 min\n",
      "Epoch: 12, G_Loss: 2.143384, D_Loss: 0.155307, time: 17.48 min\n",
      "Epoch: 13, G_Loss: 1.659532, D_Loss: 0.140714, time: 18.17 min\n",
      "Epoch: 13, G_Loss: 2.552994, D_Loss: 0.179158, time: 18.86 min\n",
      "Epoch: 14, G_Loss: 3.419054, D_Loss: 0.131843, time: 19.56 min\n",
      "Epoch: 14, G_Loss: 2.275373, D_Loss: 0.241237, time: 20.27 min\n",
      "Epoch: 15, G_Loss: 4.749281, D_Loss: 0.319012, time: 20.97 min\n",
      "Epoch: 15, G_Loss: 2.722231, D_Loss: 0.106133, time: 21.67 min\n",
      "Epoch: 16, G_Loss: 2.197176, D_Loss: 0.208043, time: 22.36 min\n",
      "Epoch: 17, G_Loss: 7.203438, D_Loss: 0.407407, time: 23.05 min\n",
      "Epoch: 17, G_Loss: 1.468626, D_Loss: 0.247553, time: 23.75 min\n",
      "Epoch: 18, G_Loss: 3.122638, D_Loss: 0.036523, time: 24.44 min\n",
      "Epoch: 18, G_Loss: 3.950133, D_Loss: 0.289656, time: 25.16 min\n",
      "Epoch: 19, G_Loss: 2.156281, D_Loss: 0.247643, time: 25.86 min\n",
      "Epoch: 19, G_Loss: 5.336246, D_Loss: 0.067165, time: 26.56 min\n",
      "Epoch: 20, G_Loss: 2.936653, D_Loss: 0.149573, time: 27.26 min\n",
      "Epoch: 20, G_Loss: 2.828897, D_Loss: 0.203106, time: 27.95 min\n",
      "Epoch: 21, G_Loss: 2.527096, D_Loss: 0.092117, time: 28.64 min\n",
      "Epoch: 21, G_Loss: 1.837494, D_Loss: 0.347327, time: 29.34 min\n",
      "Epoch: 22, G_Loss: 3.969024, D_Loss: 0.160944, time: 30.03 min\n",
      "Epoch: 22, G_Loss: 3.066271, D_Loss: 0.179989, time: 30.71 min\n",
      "Epoch: 23, G_Loss: 3.127554, D_Loss: 0.172847, time: 31.40 min\n",
      "Epoch: 23, G_Loss: 2.284766, D_Loss: 0.068363, time: 32.09 min\n",
      "Epoch: 24, G_Loss: 3.885391, D_Loss: 0.104420, time: 32.78 min\n",
      "Epoch: 25, G_Loss: 3.632239, D_Loss: 0.338416, time: 33.47 min\n",
      "Epoch: 25, G_Loss: 2.984917, D_Loss: 0.099813, time: 34.15 min\n",
      "Epoch: 26, G_Loss: 1.422875, D_Loss: 0.221556, time: 34.85 min\n",
      "Epoch: 26, G_Loss: 2.223305, D_Loss: 0.265411, time: 35.53 min\n",
      "Epoch: 27, G_Loss: 4.733689, D_Loss: 0.184349, time: 36.22 min\n",
      "Epoch: 27, G_Loss: 1.198310, D_Loss: 0.296982, time: 36.91 min\n",
      "Epoch: 28, G_Loss: 2.299609, D_Loss: 0.102922, time: 37.59 min\n",
      "Epoch: 28, G_Loss: 2.020194, D_Loss: 0.245473, time: 38.28 min\n",
      "Epoch: 29, G_Loss: 2.626917, D_Loss: 0.093577, time: 38.97 min\n",
      "Epoch: 29, G_Loss: 2.764408, D_Loss: 0.191939, time: 39.65 min\n",
      "Epoch: 30, G_Loss: 2.794676, D_Loss: 0.061848, time: 40.33 min\n",
      "Epoch: 30, G_Loss: 2.134250, D_Loss: 0.311812, time: 41.02 min\n",
      "Epoch: 31, G_Loss: 2.706366, D_Loss: 0.140458, time: 41.70 min\n",
      "Epoch: 31, G_Loss: 2.165584, D_Loss: 0.125718, time: 42.39 min\n",
      "Epoch: 32, G_Loss: 2.627359, D_Loss: 0.119974, time: 43.07 min\n",
      "Epoch: 33, G_Loss: 3.073126, D_Loss: 0.203290, time: 43.75 min\n",
      "Epoch: 33, G_Loss: 2.549251, D_Loss: 0.115049, time: 44.44 min\n",
      "Epoch: 34, G_Loss: 1.827739, D_Loss: 0.190268, time: 45.13 min\n",
      "Epoch: 34, G_Loss: 1.914093, D_Loss: 0.194664, time: 45.81 min\n",
      "Epoch: 35, G_Loss: 3.885405, D_Loss: 0.449846, time: 46.49 min\n",
      "Epoch: 35, G_Loss: 3.294616, D_Loss: 0.292796, time: 47.18 min\n",
      "Epoch: 36, G_Loss: 4.219376, D_Loss: 0.132425, time: 47.86 min\n",
      "Epoch: 36, G_Loss: 2.999398, D_Loss: 0.091779, time: 48.55 min\n",
      "Epoch: 37, G_Loss: 2.851996, D_Loss: 0.210964, time: 49.24 min\n",
      "Epoch: 37, G_Loss: 5.811800, D_Loss: 0.164025, time: 49.94 min\n",
      "Epoch: 38, G_Loss: 3.163916, D_Loss: 0.255622, time: 50.63 min\n",
      "Epoch: 38, G_Loss: 2.746959, D_Loss: 0.132586, time: 51.31 min\n",
      "Epoch: 39, G_Loss: 2.683302, D_Loss: 0.065050, time: 52.00 min\n",
      "Epoch: 39, G_Loss: 1.698859, D_Loss: 0.211097, time: 52.69 min\n",
      "Epoch: 40, G_Loss: 3.239751, D_Loss: 0.399107, time: 53.37 min\n",
      "Epoch: 41, G_Loss: 1.836177, D_Loss: 0.220654, time: 54.05 min\n",
      "Epoch: 41, G_Loss: 2.788366, D_Loss: 0.185244, time: 54.74 min\n",
      "Epoch: 42, G_Loss: 1.287527, D_Loss: 0.346806, time: 55.42 min\n",
      "Epoch: 42, G_Loss: 2.482483, D_Loss: 0.298111, time: 56.11 min\n",
      "Epoch: 43, G_Loss: 1.407902, D_Loss: 0.360707, time: 56.80 min\n",
      "Epoch: 43, G_Loss: 3.571543, D_Loss: 0.132429, time: 57.48 min\n",
      "Epoch: 44, G_Loss: 2.513263, D_Loss: 0.281993, time: 58.16 min\n",
      "Epoch: 44, G_Loss: 1.604281, D_Loss: 0.465738, time: 58.85 min\n",
      "Epoch: 45, G_Loss: 3.009822, D_Loss: 0.196947, time: 59.53 min\n",
      "Epoch: 45, G_Loss: 2.722106, D_Loss: 0.356296, time: 60.21 min\n",
      "Epoch: 46, G_Loss: 1.219153, D_Loss: 0.280650, time: 60.90 min\n",
      "Epoch: 46, G_Loss: 3.464066, D_Loss: 0.230000, time: 61.58 min\n",
      "Epoch: 47, G_Loss: 2.845118, D_Loss: 0.189346, time: 62.26 min\n",
      "Epoch: 47, G_Loss: 2.091931, D_Loss: 0.183007, time: 62.95 min\n",
      "Epoch: 48, G_Loss: 3.696795, D_Loss: 0.126401, time: 63.67 min\n",
      "Epoch: 49, G_Loss: 2.124754, D_Loss: 0.186363, time: 64.37 min\n",
      "Epoch: 49, G_Loss: 2.760675, D_Loss: 0.166983, time: 65.10 min\n",
      "Epoch: 50, G_Loss: 2.339019, D_Loss: 0.127647, time: 65.79 min\n",
      "Epoch: 50, G_Loss: 2.694742, D_Loss: 0.240126, time: 66.48 min\n",
      "Epoch: 51, G_Loss: 2.137586, D_Loss: 0.135953, time: 67.18 min\n",
      "Epoch: 51, G_Loss: 3.168533, D_Loss: 0.144586, time: 67.87 min\n",
      "Epoch: 52, G_Loss: 2.009281, D_Loss: 0.292442, time: 68.55 min\n",
      "Epoch: 52, G_Loss: 2.018701, D_Loss: 0.174893, time: 69.24 min\n",
      "Epoch: 53, G_Loss: 4.298658, D_Loss: 0.356149, time: 69.93 min\n",
      "Epoch: 53, G_Loss: 2.613778, D_Loss: 0.249559, time: 70.62 min\n",
      "Epoch: 54, G_Loss: 2.264991, D_Loss: 0.192828, time: 71.32 min\n",
      "Epoch: 54, G_Loss: 3.553437, D_Loss: 0.113100, time: 72.01 min\n",
      "Epoch: 55, G_Loss: 2.786555, D_Loss: 0.159672, time: 72.71 min\n",
      "Epoch: 55, G_Loss: 1.524386, D_Loss: 0.179993, time: 73.40 min\n",
      "Epoch: 56, G_Loss: 4.257794, D_Loss: 0.134934, time: 74.09 min\n",
      "Epoch: 57, G_Loss: 2.252232, D_Loss: 0.226703, time: 74.78 min\n",
      "Epoch: 57, G_Loss: 1.958877, D_Loss: 0.177903, time: 75.48 min\n",
      "Epoch: 58, G_Loss: 2.702992, D_Loss: 0.193007, time: 76.20 min\n",
      "Epoch: 58, G_Loss: 3.176458, D_Loss: 0.078599, time: 76.93 min\n",
      "Epoch: 59, G_Loss: 3.530770, D_Loss: 0.112487, time: 77.63 min\n",
      "Epoch: 59, G_Loss: 2.626287, D_Loss: 0.089818, time: 78.36 min\n",
      "Epoch: 60, G_Loss: 2.630228, D_Loss: 0.108404, time: 79.09 min\n",
      "Epoch: 60, G_Loss: 2.142850, D_Loss: 0.258327, time: 79.82 min\n",
      "Epoch: 61, G_Loss: 2.896483, D_Loss: 0.251811, time: 80.54 min\n",
      "Epoch: 61, G_Loss: 2.242471, D_Loss: 0.103874, time: 81.25 min\n",
      "Epoch: 62, G_Loss: 2.954499, D_Loss: 0.162642, time: 81.96 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25252/1638136199.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmodel_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mba_si\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mout_gen\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmodel_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mout_dis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_dis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25252/3053747750.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    168\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2279\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2281\u001b[1;33m     return torch.batch_norm(\n\u001b[0m\u001b[0;32m   2282\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2283\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_count = 0\n",
    "start_time = time.time()\n",
    "model_dis.train()\n",
    "model_gen.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        ba_si = xb.size(0)\n",
    "        \n",
    "        xb = xb.to(device)\n",
    "        yb_real = torch.Tensor(ba_si, 1).fill_(1.0).to(device)\n",
    "        yb_fake = torch.Tensor(ba_si, 1).fill_(0.0).to(device)\n",
    "        \n",
    "        model_gen.zero_grad()\n",
    "        noise = torch.randn(ba_si, nz, device=device)\n",
    "        out_gen =model_gen(noise)\n",
    "        out_dis = model_dis(out_gen)\n",
    "        \n",
    "        loss_gen = loss_func(out_dis, yb_real)\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        model_dis.zero_grad()\n",
    "        \n",
    "        out_real = model_dis(xb)\n",
    "        out_fake = model_dis(out_gen.detach())\n",
    "        loss_real = loss_func(out_real, yb_real)\n",
    "        loss_fake = loss_func(out_fake, yb_fake)\n",
    "        loss_dis = (loss_real + loss_fake) / 2\n",
    "        \n",
    "        loss_dis.backward()\n",
    "        opt_dis.step()\n",
    "        \n",
    "        loss_history['gen'].append(loss_gen.item())\n",
    "        loss_history['dis'].append(loss_dis.item())\n",
    "        \n",
    "        batch_count +=1\n",
    "        if batch_count % 1000 ==0:\n",
    "            print('Epoch: %.0f, G_Loss: %.6f, D_Loss: %.6f, time: %.2f min' %(epoch, loss_gen.item(),\n",
    "                                                                             loss_dis.item(), (time.time() - start_time)/60))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3bfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
